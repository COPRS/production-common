spring:
  main:
    banner-mode: "off"
  application:
    name: s1pdgs-contingency-ingestion-trigger

# Logging
logging:
  config: ${log4j_config:log/log4j2.yml}

process:
  # hostname of the container (derived by 'hostname' command)
  hostname: ${HOSTNAME}

# Ingestion Trigger config
ingestion-trigger:
  # Maximum number of consecutive retries for publishing of ingestion jobs (defaults to 10)
  publish-max-retries: {{if .Values.contingencyIngestion }}{{.Values.contingencyIngestion.publishMaxRetries | default "10" }}{{else}} 10 {{end}}
  # Time between retries after unsuccessful publishing of ingestion jobs in millis (defaults to 10000)
  publish-tempo-retry-ms: {{if .Values.contingencyIngestion }}{{.Values.contingencyIngestion.publishTempoRetryMs | default "10000" }}{{else}} 10000 {{end}}
  # Pickup point polling interval in millis
  polling-interval-ms: 10000
  
  polling:
    # Handling of EDRS sessions
      # Pickup path
    - directory: file:///data/inbox/MPS_/
      # Filename regex for matching files
      matchRegex: ^([A-Za-z_]{4}/)?([0-9A-Za-z_]{2})([0-9A-Za-z_]{1})/([0-9A-Za-z_]+)/(ch[0|_]?[1-2]/)?(DCS_[0-9]{2}_([a-zA-Z0-9_]*)_ch([12])_(DSDB|DSIB).*\.(raw|aisp|xml|RAW|AISP|XML))$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: EDRS_SESSION
      # Name of the station for EDRS sessions (4-letter code)
      stationName: MPS_
      # Pickup path
    - directory: file:///data/inbox/WILE/
      # Filename regex for matching files
      matchRegex: ^([A-Za-z_]{4}/)?([0-9A-Za-z_]{2})([0-9A-Za-z_]{1})/([0-9A-Za-z_]+)/(ch[0|_]?[1-2]/)?(DCS_[0-9]{2}_([a-zA-Z0-9_]*)_ch([12])_(DSDB|DSIB).*\.(raw|aisp|xml|RAW|AISP|XML))$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: EDRS_SESSION
      # Name of the station for EDRS sessions (4-letter code)
      stationName: WILE
      # Pickup path
    - directory: file:///data/inbox/MTI_/
      # Filename regex for matching files
      matchRegex: ^([A-Za-z_]{4}/)?([0-9A-Za-z_]{2})([0-9A-Za-z_]{1})/([0-9A-Za-z_]+)/(ch[0|_]?[1-2]/)?(DCS_[0-9]{2}_([a-zA-Z0-9_]*)_ch([12])_(DSDB|DSIB).*\.(raw|aisp|xml|RAW|AISP|XML))$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: EDRS_SESSION
      # Name of the station for EDRS sessions (4-letter code)
      stationName: MTI_
      # Pickup path
    - directory: file:///data/inbox/SGS_/
      # Filename regex for matching files
      matchRegex: ^([A-Za-z_]{4}/)?([0-9A-Za-z_]{2})([0-9A-Za-z_]{1})/([0-9A-Za-z_]+)/(ch[0|_]?[1-2]/)?(DCS_[0-9]{2}_([a-zA-Z0-9_]*)_ch([12])_(DSDB|DSIB).*\.(raw|aisp|xml|RAW|AISP|XML))$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: EDRS_SESSION
      # Name of the station for EDRS sessions (4-letter code)
      stationName: SGS_
      # Pickup path
    - directory: file:///data/inbox/NSG_/
      matchRegex: ^([A-Za-z_]{4}/)?([0-9A-Za-z_]{2})([0-9A-Za-z_]{1})/([0-9A-Za-z_]+)/(ch[0|_]?[1-2]/)?(DCS_[0-9]{2}_([a-zA-Z0-9_]*)_ch([12])_(DSDB|DSIB).*\.(raw|aisp|xml|RAW|AISP|XML))$
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      topic: t-pdgs-contingency-ingestion-jobs
      family: EDRS_SESSION
      stationName: NSG_
      # Pickup path
    - directory: file:///data/inbox/INU_/
      # Filename regex for matching files
      matchRegex: ^([A-Za-z_]{4}/)?([0-9A-Za-z_]{2})([0-9A-Za-z_]{1})/([0-9A-Za-z_]+)/(ch[0|_]?[1-2]/)?(DCS_[0-9]{2}_([a-zA-Z0-9_]*)_ch([12])_(DSDB|DSIB).*\.(raw|aisp|xml|RAW|AISP|XML))$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: EDRS_SESSION
      # Name of the station for EDRS sessions (4-letter code)
      stationName: INU_
    # Handling of AUX files
      # Pickup path
    - directory: file:///data/inbox/AUX/
      # Filename regex for matching files
      matchRegex: ^[0-9a-zA-Z][0-9a-zA-Z][0-9a-zA-Z_]_((OPER|TEST|REPR)_)?(AMH_ERRMAT|AMV_ERRMAT|AUX_CAL|AUX_ICE|AUX_INS|AUX_OBMEMC|AUX_POEORB|AUX_PP1|AUX_PP2|AUX_PREORB|AUX_RESORB|AUX_SCS|AUX_WAV|AUX_WND|MPL_ORBPRE|MPL_ORBRES|MPL_ORBSCT|MSK_EW_SLC|MSK__LAND|MSK_OCEAN_|MSK_OVRPAS)_[^/]*\.(xml|XML|EOF|SAFE)$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: AUXILIARY_FILE
    # Handling of AUX files (zipped)
      # Pickup path
    - directory: file:///data/inbox/AUX_ZIP/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.(zip|ZIP)$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: AUXILIARY_FILE_ZIP
    # Handling of L0 Segments
      # Pickup path
    - directory: file:///data/inbox/L0_SEGMENT/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L0_SEGMENT
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of L0 Slices
      # Pickup path
    - directory: file:///data/inbox/L0_SLICE/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L0_SLICE
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of L0 Acns
      # Pickup path
    - directory: file:///data/inbox/L0_ACN/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L0_ACN
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of L1 Slices
      # Pickup path
    - directory: file:///data/inbox/L1_SLICE/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L1_SLICE
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of L1 Acns
      # Pickup path
    - directory: file:///data/inbox/L1_ACN/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L1_ACN
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of L2 Slices
      # Pickup path
    - directory: file:///data/inbox/L2_SLICE/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L2_SLICE
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of L2 Acns
      # Pickup path
    - directory: file:///data/inbox/L2_ACN/
      # Filename regex for matching files
      matchRegex: ^S1[ABCD].*\.SAFE$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: L2_ACN
      # Processing mode to persist in the metadata of the ingested file
      mode: NOMINAL
      # Timeliness to persist in the metadata of the ingested file
      timeliness: FAST24
    # Handling of plan and report files (zipped)
      # Pickup path
    - directory: file:///data/inbox/Plans_Reports_ZIP/
      # Filename regex for matching files
      matchRegex: ^(S1[ABCD_]_OPER_REP_MP_MP__PDMC_|S1[AB_]OPER_REP_MP_MPPDMC|S1[ABCD]_OPER_MPL_SP.{4}_PDMC_|S1[ABCD_]_OPER_MPL_FS.{4}_PDMC_|S1[ABCD]_OPER_REP_PASS_[1-9]_.{4}_|S[12]__OPER_SRA_EDRS_[AC]_PDMC_|EDR_OPER_MPL_RQ[1-9]_O[AC]_|EDR_OPER_MPL_[LM]AS_O[AC]_|EDR_OPER_MPL_CR[1-9]_O[AC]_|EDR_OPER_MPL_SS[1-9]_O[AC]_|EDR_OPER_MPL_ER[1-9]_O[AC]_|EDR_OPER_SER_SR[1-9]_O[AC]_|S1[ABCD]_OPER_MPL_ORBOEM_|EDR_OPER_MPL_GOB_P[AC]_|EDR_OPER_MPL_GOB_R[AC]_|S1[ABCD]_OPER_REP__SUP___|S1[ABCD]_OPER_REP_STNACQ_.{4}_|S1[ABCD_]_OPER_REP_STNUNV_.{4}_|S[123][ABCD_]_OPER_SRA_BANSEG_PDMC_|S1[ABCD]_OPER_TLM__REQ_[A-O]_|S1[ABCD]_OPER_REP__SMPR__|S1[ABCD]_OPER_MPL__SSC___|S1[ABCD]_OPER_TLM__PSCAT_|S1[ABCD]_OPER_MPL_OCMSAR_|S1[ABCD]_OPER_REP__MACP__|S1[ABCD]_OPER_REP__MCSF__|S1[ABCD]_OPER_MPL__NPPF__|S1[ABCD]_OPER_MPL__NPIF__|S1[ABCD]_OPER_REP_NPIFCC_|S[123][ABCD_]_OPER_SRA_GSUNAV_PDMC_|S1[ABCD]_OPER_OBS_MIMG___|S1[ABCD]_OPER_AUX_RDB____MPC__|S1[ABCD]_OPER_MPL_SESDB[ABCD]_|S1[ABCD]_OPER_REP__CHF___|S1[AB]_OPER_REP__FCHF__|S1[AB]_OPER_AM[VH_]_FAILUR_MPC__|S1[AB]_OPER_AUX_QCSTDB_|S1[AB_]_OPER_REP_QC...._MPC__|S1[AB]OPER_REPSUP__|S1[AB]OPER_REPMACP_).*(\.xml|\.XML|\.EOF|\.TGZ)?\.(zip|ZIP)$
      # Filename regex for ignoring files
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      # Kafka topic to put the ingestion jobs for the worker
      topic: t-pdgs-contingency-ingestion-jobs
      # Product family accociated with the file to be ingested
      family: PLAN_AND_REPORT_ZIP
    # Handling of Granules      
    - directory: file:///data/inbox/s3_granules/
      # e.g. S3A_OL_0_EFR__G_20040703T005217_20040703T005417_20150424T160529___________________WER_D_______.ISIP
      matchRegex: ^S3[AB_]_([0-9a-zA-Z_]{11})_([0-9]{8}T[0-9]{6})_([0-9]{8}T[0-9]{6})_([0-9]{8}T[0-9]{6})_((([0-9a-zA-Z_]{4})_([0-9]{3})_([0-9]{3})_(([0-9]{4})|(_{4})))|([0-9a-zA-Z_]{17}))_([0-9a-zA-Z_]{3})_(((O|F|R|D|_)_(NR|NT|ST|AL|__)_([a-zA-Z0-9_]{3}))|_{8})\.ISIP$
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      topic: t-pdgs-contingency-ingestion-jobs
      family: S3_GRANULES
    # Handling of AUX files
    - directory: file:///data/inbox/s3_aux/
      # e.g. S3A_AX___OSF_AX_20040702T203000_20040704T042158_20171130T082119___________________WER_D_AL____.SEN3
      matchRegex: ^S3[AB_]_([0-9a-zA-Z_]{9})AX_([0-9]{8}T[0-9]{6})_([0-9]{8}T[0-9]{6})_([0-9]{8}T[0-9]{6})_(_{17})_([0-9a-zA-Z_]{3})_(((O|F|R|D|_)_(NR|NT|ST|SN|NS|NN|AL|__)_([a-zA-Z0-9_]{3}))|_{8})\.SEN3$
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      topic: t-pdgs-contingency-ingestion-jobs
      family: S3_AUX
    # Handling of S3 LO files
    - directory: file:///data/inbox/s3_l0/
      # e.g. S3B_OL_0_EFR____20200121T023043_20200121T023243_20201012T161332_0119_034_317______LN1_O_NR_002.SEN3
      matchRegex: ^S3[AB_]_.*\.SEN3$
      ignoreRegex: (^\..*|.*\.tmp$|db.*|^lost\+found$)
      topic: t-pdgs-contingency-ingestion-jobs
      family: S3_L0

# Kafka config
kafka:
  # host:port to use for establishing the initial connection to the Kafka cluster
  bootstrap-servers: ${kafka_bootstrap-servers}
  # hostname
  hostname: ${HOSTNAME}
  # Topic name for the errors
  error-topic: ${kafka_topic_errors}
  # Kafka Producer config
  producer:
    # When greater than zero, enables retrying of failed sends
    max-retries: 10

# MongoDB config
mongodb:
  host: ${mongo_db_host:localhost}
  port: ${mongo_db_port:27017}
  database: ${mongo_db_database:s1pdgs}

# Application status configuration
status:
  # (fixed delay) period in milliseconds between 2 check if application shall be stopped or not, default is 3000
  delete-fixed-delay-ms: 3000
  # The number of consecutive processing errors leading to the state FATALERROR, default is 100
  max-error-counter-processing: 3
  # The number of consecutive MQI errors leading to the state FATALERROR, default is 100
  max-error-counter-mqi: 30

